/** * @license * Copyright Google LLC All Rights Reserved. * * Use of this source code is governed by an MIT-style license that can be * found in the LICENSE file at https://angular.io/license */import {getHtmlTagDefinition} from '../../src/ml_parser/html_tags';import {TokenError, tokenize, TokenizeOptions, TokenizeResult} from '../../src/ml_parser/lexer';import {Token, TokenType} from '../../src/ml_parser/tokens';import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_util';{  describe('HtmlLexer', () => {    describe('line/column numbers', () => {ould work without newlines', () => {ct(tokenizeAndHumanizeLineColumn('<t>a</t>')).toEqual([[TokenType.TAG_OPEN_START, '0:0'],[TokenType.TAG_OPEN_END, '0:2'],[TokenType.TEXT, '0:3'],[TokenType.TAG_CLOSE, '0:4'],[TokenType.EOF, '0:8'],ould work with one newline', () => {ct(tokenizeAndHumanizeLineColumn('<t>\na</t>')).toEqual([[TokenType.TAG_OPEN_START, '0:0'],[TokenType.TAG_OPEN_END, '0:2'],[TokenType.TEXT, '0:3'],[TokenType.TAG_CLOSE, '1:1'],[TokenType.EOF, '1:5'],ould work with multiple newlines', () => {ct(tokenizeAndHumanizeLineColumn('<t\n>\na</t>')).toEqual([[TokenType.TAG_OPEN_START, '0:0'],[TokenType.TAG_OPEN_END, '1:0'],[TokenType.TEXT, '1:1'],[TokenType.TAG_CLOSE, '2:1'],[TokenType.EOF, '2:5'],ould work with CR and LF', () => {ct(tokenizeAndHumanizeLineColumn('<t\n>\r\na\r</t>')).toEqual([[TokenType.TAG_OPEN_START, '0:0'],[TokenType.TAG_OPEN_END, '1:0'],[TokenType.TEXT, '1:1'],[TokenType.TAG_CLOSE, '2:1'],[TokenType.EOF, '2:5'],ould skip over leading trivia for source-span start', () => {ct( tokenizeAndHumanizeFullStart('<t>\n \t a</t>', {leadingTriviaChars: ['\n', ' ', '\t']})) .toEqual([   [TokenType.TAG_OPEN_START, '0:0', '0:0'],   [TokenType.TAG_OPEN_END, '0:2', '0:2'],   [TokenType.TEXT, '1:3', '0:3'],   [TokenType.TAG_CLOSE, '1:4', '1:4'],   [TokenType.EOF, '1:8', '1:8'], ]);    });    describe('content ranges', () => {ould only process the text within the range', () => {ct(tokenizeAndHumanizeSourceSpans(    'pre 1\npre 2\npre 3 `line 1\nline 2\nline 3` post 1\n post 2\n post 3',    {range: {startPos: 19, startLine: 2, startCol: 7, endPos: 39}})) .toEqual([   [TokenType.TEXT, 'line 1\nline 2\nline 3'],   [TokenType.EOF, ''], ]);ould take into account preceding (non-processed) lines and columns', () => {ct(tokenizeAndHumanizeLineColumn(    'pre 1\npre 2\npre 3 `line 1\nline 2\nline 3` post 1\n post 2\n post 3',    {range: {startPos: 19, startLine: 2, startCol: 7, endPos: 39}})) .toEqual([   [TokenType.TEXT, '2:7'],   [TokenType.EOF, '4:6'], ]);    });    describe('comments', () => {ould parse comments', () => {ct(tokenizeAndHumanizeParts('<!--t\ne\rs\r\nt-->')).toEqual([[TokenType.COMMENT_START],[TokenType.RAW_TEXT, 't\ne\ns\nt'],[TokenType.COMMENT_END],[TokenType.EOF],ould store the locations', () => {ct(tokenizeAndHumanizeSourceSpans('<!--t\ne\rs\r\nt-->')).toEqual([[TokenType.COMMENT_START, '<!--'],[TokenType.RAW_TEXT, 't\ne\rs\r\nt'],[TokenType.COMMENT_END, '-->'],[TokenType.EOF, ''],ould report <!- without -', () => {ct(tokenizeAndHumanizeErrors('<!-a')).toEqual([[TokenType.COMMENT_START, 'Unexpected character "a"', '0:3']ould report missing end comment', () => {ct(tokenizeAndHumanizeErrors('<!--')).toEqual([[TokenType.RAW_TEXT, 'Unexpected character "EOF"', '0:4']ould accept comments finishing by too many dashes (even number)', () => {ct(tokenizeAndHumanizeSourceSpans('<!-- test ---->')).toEqual([[TokenType.COMMENT_START, '<!--'],[TokenType.RAW_TEXT, ' test --'],[TokenType.COMMENT_END, '-->'],[TokenType.EOF, ''],ould accept comments finishing by too many dashes (odd number)', () => {ct(tokenizeAndHumanizeSourceSpans('<!-- test --->')).toEqual([[TokenType.COMMENT_START, '<!--'],[TokenType.RAW_TEXT, ' test -'],[TokenType.COMMENT_END, '-->'],[TokenType.EOF, ''],    });    describe('doctype', () => {ould parse doctypes', () => {ct(tokenizeAndHumanizeParts('<!doctype html>')).toEqual([[TokenType.DOC_TYPE, 'doctype html'],[TokenType.EOF],ould store the locations', () => {ct(tokenizeAndHumanizeSourceSpans('<!doctype html>')).toEqual([[TokenType.DOC_TYPE, '<!doctype html>'],[TokenType.EOF, ''],ould report missing end doctype', () => {ct(tokenizeAndHumanizeErrors('<!')).toEqual([[TokenType.DOC_TYPE, 'Unexpected character "EOF"', '0:2']    });    describe('CDATA', () => {ould parse CDATA', () => {ct(tokenizeAndHumanizeParts('<![CDATA[t\ne\rs\r\nt]]>')).toEqual([[TokenType.CDATA_START],[TokenType.RAW_TEXT, 't\ne\ns\nt'],[TokenType.CDATA_END],[TokenType.EOF],ould store the locations', () => {ct(tokenizeAndHumanizeSourceSpans('<![CDATA[t\ne\rs\r\nt]]>')).toEqual([[TokenType.CDATA_START, '<![CDATA['],[TokenType.RAW_TEXT, 't\ne\rs\r\nt'],[TokenType.CDATA_END, ']]>'],[TokenType.EOF, ''],ould report <![ without CDATA[', () => {ct(tokenizeAndHumanizeErrors('<![a')).toEqual([[TokenType.CDATA_START, 'Unexpected character "a"', '0:3']ould report missing end cdata', () => {ct(tokenizeAndHumanizeErrors('<![CDATA[')).toEqual([[TokenType.RAW_TEXT, 'Unexpected character "EOF"', '0:9']    });    describe('open tags', () => {ould parse open tags without prefix', () => {ct(tokenizeAndHumanizeParts('<test>')).toEqual([[TokenType.TAG_OPEN_START, '', 'test'],[TokenType.TAG_OPEN_END],[TokenType.EOF],ould parse namespace prefix', () => {ct(tokenizeAndHumanizeParts('<ns1:test>')).toEqual([[TokenType.TAG_OPEN_START, 'ns1', 'test'],[TokenType.TAG_OPEN_END],[TokenType.EOF],ould parse void tags', () => {ct(tokenizeAndHumanizeParts('<test/>')).toEqual([[TokenType.TAG_OPEN_START, '', 'test'],[TokenType.TAG_OPEN_END_VOID],[TokenType.EOF],ould allow whitespace after the tag name', () => {ct(tokenizeAndHumanizeParts('<test >')).toEqual([[TokenType.TAG_OPEN_START, '', 'test'],[TokenType.TAG_OPEN_END],[TokenType.EOF],ould store the locations', () => {ct(tokenizeAndHumanizeSourceSpans('<test>')).toEqual([[TokenType.TAG_OPEN_START, '<test'],[TokenType.TAG_OPEN_END, '>'],[TokenType.EOF, ''],be('tags', () => {terminated with EOF', () => {expect(tokenizeAndHumanizeSourceSpans('<div')).toEqual([ [TokenType.INCOMPLETE_TAG_OPEN, '<div'], [TokenType.EOF, ''],]);after tag name', () => {expect(tokenizeAndHumanizeSourceSpans('<div<span><div</span>')).toEqual([ [TokenType.INCOMPLETE_TAG_OPEN, '<div'], [TokenType.TAG_OPEN_START, '<span'], [TokenType.TAG_OPEN_END, '>'], [TokenType.INCOMPLETE_TAG_OPEN, '<div'], [TokenType.TAG_CLOSE, '</span>'], [TokenType.EOF, ''],]);in attribute', () => {expect(tokenizeAndHumanizeSourceSpans('<div class="hi" sty<span></span>')).toEqual([ [TokenType.INCOMPLETE_TAG_OPEN, '<div'], [TokenType.ATTR_NAME, 'class'], [TokenType.ATTR_QUOTE, '"'], [TokenType.ATTR_VALUE_TEXT, 'hi'], [TokenType.ATTR_QUOTE, '"'], [TokenType.ATTR_NAME, 'sty'], [TokenType.TAG_OPEN_START, '<span'], [TokenType.TAG_OPEN_END, '>'], [TokenType.TAG_CLOSE, '</span>'], [TokenType.EOF, ''],]);after quote', () => {expect(tokenizeAndHumanizeSourceSpans('<div "<span></span>')).toEqual([ [TokenType.INCOMPLETE_TAG_OPEN, '<div'], [TokenType.TEXT, '"'], [TokenType.TAG_OPEN_START, '<span'], [TokenType.TAG_OPEN_END, '>'], [TokenType.TAG_CLOSE, '</span>'], [TokenType.EOF, ''],]);    });    describe('attributes', () => {ould parse attributes without prefix', () => {ct(tokenizeAndHumanizeParts('<t a>')).toEqual([[TokenType.TAG_OPEN_START, '', 't'],[TokenType.ATTR_NAME, '', 'a'],[TokenType.TAG_OPEN_END],[TokenType.EOF],ould parse attributes with interpolation', () => {ct(tokenizeAndHumanizeParts('<t a="{{v}}" b="s{{m}}e" c="s{{m//c}}e">')).toEqual([[TokenType.TAG_OPEN_START, '', 't'],[TokenType.ATTR_NAME, '', 'a'],[TokenType.ATTR_QUOTE, '"'],[TokenType.ATTR_VALUE_TEXT, ''],[TokenType.ATTR_VALUE_INTERPOLATION, '{{', 'v', '}}'],[TokenType.ATTR_VALUE_TEXT, ''],[TokenType.ATTR_QUOTE, '"'],[TokenType.ATTR_NAME, '', 'b'],[TokenType.ATTR_QUOTE, '"'],[TokenType.ATTR_VALUE_TEXT, 's'],[TokenType.ATTR_VALUE_INTERPOLATION, '{{', 'm', '}}'],[TokenType.ATTR_VALUE_TEXT, 'e'],[TokenType.ATTR_QUOTE, '"'],[TokenType.ATTR_NAME, '', 'c'],[TokenType.ATTR_QUOTE, '"'],[TokenType.ATTR_VALUE_TEXT, 's'],[TokenType.ATTR_VALUE_INTERPOLATION, '{{', 'm//c', '}}'],[TokenType.ATTR_VALUE_TEXT, 'e'],[TokenType.ATTR_QUOTE, '"'],[TokenType.TAG_OPEN_END],[TokenType.EOF],ould end interpolation on an unescaped matching quote', () => {ct(tokenizeAndHumanizeParts('<t a="{{ a \\" \' b ">')).toEqual([[TokenType.TAG_OPEN_START, '', 't'],[TokenType.ATTR_NAME, '', 'a'],[TokenType.ATTR_QUOTE, '"'],[TokenType.ATTR_VALUE_TEXT, ''],[TokenType.ATTR_VALUE_INTERPOLATION, '{{', ' a \\" \' b '],[TokenType.ATTR_VALUE_TEXT, ''],[TokenType.ATTR_QUOTE, '"'],[TokenType.TAG_OPEN_END],[TokenType.EOF],ct(tokenizeAndHumanizeParts('<t a=\'{{ a " \\\' b \'>')).toEqual([[TokenType.TAG_OPEN_START, '', 't'],[TokenType.ATTR_NAME, '', 'a'],[TokenType.ATTR_QUOTE, '\''],[TokenType.ATTR_VALUE_TEXT, ''],[TokenType.ATTR_VALUE_INTERPOLATION, '{{', ' a " \\\' b '],[TokenType.ATTR_VALUE_TEXT, ''],[TokenType.ATTR_QUOTE, '\''],[TokenType.TAG_OPEN_END],[TokenType.EOF],ould parse attributes with prefix', () => {ct(tokenizeAndHumanizeParts('<t ns1:a>')).toEqual([[TokenType.TAG_OPEN_START, '', 't'],[TokenType.ATTR_NAME, 'ns1', 'a'],[TokenType.TAG_OPEN_END],[TokenType.EOF],ould parse attributes whose prefix is not valid', () => {ct(tokenizeAndHumanizeParts('<t (ns1:a)>')).toEqual([[TokenType.TAG_OPEN_START, '', 't'],[TokenType.ATTR_NAME, '', '(ns1:a)'],[TokenType.TAG_OPEN_END],[TokenType.EOF],ould parse attributes with single quote value', () => {ct(tokenizeAndHumanizeParts('<t a=\'b\'>')).toEqual([[TokenType.TAG_OPEN_START, '', 't'],[TokenType.ATTR_NAME, '', 'a'],[TokenType.ATTR_QUOTE, '\''],[TokenType.ATTR_VALUE_TEXT, 'b'],[TokenType.ATTR_QUOTE, '\''],[TokenType.TAG_OPEN_END],[TokenType.EOF],ould parse attributes with double quote value', () => {ct(tokenizeAndHumanizeParts('<t a="b">')).toEqual([[TokenType.TAG_OPEN_START, '', 't'],[TokenType.ATTR_NAME, '', 'a'],[TokenType.ATTR_QUOTE, '"'],[TokenType.ATTR_VALUE_TEXT, 'b'],[TokenType.ATTR_QUOTE, '"'],[TokenType.TAG_OPEN_END],[TokenType.EOF],ould parse attributes with unquoted value', () => {ct(tokenizeAndHumanizeParts('<t a=b>')).toEqual([[TokenType.TAG_OPEN_START, '', 't'],[TokenType.ATTR_NAME, '', 'a'],[TokenType.ATTR_VALUE_TEXT, 'b'],[TokenType.TAG_OPEN_END],[TokenType.EOF],ould parse attributes with unquoted interpolation value', () => {ct(tokenizeAndHumanizeParts('<a a={{link.text}}>')).toEqual([[TokenType.TAG_OPEN_START, '', 'a'],[TokenType.ATTR_NAME, '', 'a'],[TokenType.ATTR_VALUE_TEXT, ''],[TokenType.ATTR_VALUE_INTERPOLATION, '{{', 'link.text', '}}'],[TokenType.ATTR_VALUE_TEXT, ''],[TokenType.TAG_OPEN_END],[TokenType.EOF],ould parse bound inputs with expressions containing newlines', () => {ct(tokenizeAndHumanizeParts(`<app-componentr]="[t: 'some text',url:'//www.google.com'},t:'other text',url:'//www.google.com'}]">`)) .toEqual([   [TokenType.TAG_OPEN_START, '', 'app-component'],   [TokenType.ATTR_NAME, '', '[attr]'],   [TokenType.ATTR_QUOTE, '"'],   [ TokenType.ATTR_VALUE_TEXT, '[\n' +     't: \'some text\',url:\'//www.google.com\'},\n' +     't:\'other text\',url:\'//www.google.com\'}]'   ],   [TokenType.ATTR_QUOTE, '"'],   [TokenType.TAG_OPEN_END],   [TokenType.EOF], ]);ould parse attributes with empty quoted value', () => {ct(tokenizeAndHumanizeParts('<t a="">')).toEqual([[TokenType.TAG_OPEN_START, '', 't'],[TokenType.ATTR_NAME, '', 'a'],[TokenType.ATTR_QUOTE, '"'],[TokenType.ATTR_VALUE_TEXT, ''],[TokenType.ATTR_QUOTE, '"'],[TokenType.TAG_OPEN_END],[TokenType.EOF],ould parse bound inputs with expressions containing newlines', () => {ct(tokenizeAndHumanizeParts(`<app-componentr]="[t: 'some text',url:'//www.google.com'},t:'other text',url:'//www.google.com'}]">`)) .toEqual([   [TokenType.TAG_OPEN_START, '', 'app-component'],   [TokenType.ATTR_NAME, '', '[attr]'],   [TokenType.ATTR_QUOTE, '"'],   [ TokenType.ATTR_VALUE_TEXT, '[\n' +     't: \'some text\',url:\'//www.google.com\'},\n' +     't:\'other text\',url:\'//www.google.com\'}]'   ],   [TokenType.ATTR_QUOTE, '"'],   [TokenType.TAG_OPEN_END],   [TokenType.EOF], ]);ould allow whitespace', () => {ct(tokenizeAndHumanizeParts('<t a = b >')).toEqual([[TokenType.TAG_OPEN_START, '', 't'],[TokenType.ATTR_NAME, '', 'a'],[TokenType.ATTR_VALUE_TEXT, 'b'],[TokenType.TAG_OPEN_END],[TokenType.EOF],ould parse attributes with entities in values', () => {ct(tokenizeAndHumanizeParts('<t a="&#65;&#x41;">')).toEqual([[TokenType.TAG_OPEN_START, '', 't'],[TokenType.ATTR_NAME, '', 'a'],[TokenType.ATTR_QUOTE, '"'],[TokenType.ATTR_VALUE_TEXT, ''],[TokenType.ENCODED_ENTITY, 'A', '&#65;'],[TokenType.ATTR_VALUE_TEXT, ''],[TokenType.ENCODED_ENTITY, 'A', '&#x41;'],[TokenType.ATTR_VALUE_TEXT, ''],[TokenType.ATTR_QUOTE, '"'],[TokenType.TAG_OPEN_END],[TokenType.EOF],ould not decode entities without trailing ";"', () => {ct(tokenizeAndHumanizeParts('<t a="&amp" b="c&&d">')).toEqual([[TokenType.TAG_OPEN_START, '', 't'],[TokenType.ATTR_NAME, '', 'a'],[TokenType.ATTR_QUOTE, '"'],[TokenType.ATTR_VALUE_TEXT, '&amp'],[TokenType.ATTR_QUOTE, '"'],[TokenType.ATTR_NAME, '', 'b'],[TokenType.ATTR_QUOTE, '"'],[TokenType.ATTR_VALUE_TEXT, 'c&&d'],[TokenType.ATTR_QUOTE, '"'],[TokenType.TAG_OPEN_END],[TokenType.EOF],ould parse attributes with "&" in values', () => {ct(tokenizeAndHumanizeParts('<t a="b && c &">')).toEqual([[TokenType.TAG_OPEN_START, '', 't'],[TokenType.ATTR_NAME, '', 'a'],[TokenType.ATTR_QUOTE, '"'],[TokenType.ATTR_VALUE_TEXT, 'b && c &'],[TokenType.ATTR_QUOTE, '"'],[TokenType.TAG_OPEN_END],[TokenType.EOF],ould parse values with CR and LF', () => {ct(tokenizeAndHumanizeParts('<t a=\'t\ne\rs\r\nt\'>')).toEqual([[TokenType.TAG_OPEN_START, '', 't'],[TokenType.ATTR_NAME, '', 'a'],[TokenType.ATTR_QUOTE, '\''],[TokenType.ATTR_VALUE_TEXT, 't\ne\ns\nt'],[TokenType.ATTR_QUOTE, '\''],[TokenType.TAG_OPEN_END],[TokenType.EOF],ould store the locations', () => {ct(tokenizeAndHumanizeSourceSpans('<t a=b>')).toEqual([[TokenType.TAG_OPEN_START, '<t'],[TokenType.ATTR_NAME, 'a'],[TokenType.ATTR_VALUE_TEXT, 'b'],[TokenType.TAG_OPEN_END, '>'],[TokenType.EOF, ''],ould report missing closing single quote', () => {ct(tokenizeAndHumanizeErrors('<t a=\'b>')).toEqual([[TokenType.ATTR_VALUE_TEXT, 'Unexpected character "EOF"', '0:8'],ould report missing closing double quote', () => {ct(tokenizeAndHumanizeErrors('<t a="b>')).toEqual([[TokenType.ATTR_VALUE_TEXT, 'Unexpected character "EOF"', '0:8'],    });    describe('closing tags', () => {ould parse closing tags without prefix', () => {ct(tokenizeAndHumanizeParts('</test>')).toEqual([[TokenType.TAG_CLOSE, '', 'test'],[TokenType.EOF],ould parse closing tags with prefix', () => {ct(tokenizeAndHumanizeParts('</ns1:test>')).toEqual([[TokenType.TAG_CLOSE, 'ns1', 'test'],[TokenType.EOF],ould allow whitespace', () => {ct(tokenizeAndHumanizeParts('</ test >')).toEqual([[TokenType.TAG_CLOSE, '', 'test'],[TokenType.EOF],ould store the locations', () => {ct(tokenizeAndHumanizeSourceSpans('</test>')).toEqual([[TokenType.TAG_CLOSE, '</test>'],[TokenType.EOF, ''],ould report missing name after </', () => {ct(tokenizeAndHumanizeErrors('</')).toEqual([[TokenType.TAG_CLOSE, 'Unexpected character "EOF"', '0:2']ould report missing >', () => {ct(tokenizeAndHumanizeErrors('</test')).toEqual([[TokenType.TAG_CLOSE, 'Unexpected character "EOF"', '0:6']    });    describe('entities', () => {ould parse named entities', () => {ct(tokenizeAndHumanizeParts('a&amp;b')).toEqual([[TokenType.TEXT, 'a'],[TokenType.ENCODED_ENTITY, '&', '&amp;'],[TokenType.TEXT, 'b'],[TokenType.EOF],ould parse hexadecimal entities', () => {ct(tokenizeAndHumanizeParts('&#x41;&#X41;')).toEqual([[TokenType.TEXT, ''],[TokenType.ENCODED_ENTITY, 'A', '&#x41;'],[TokenType.TEXT, ''],[TokenType.ENCODED_ENTITY, 'A', '&#X41;'],[TokenType.TEXT, ''],[TokenType.EOF],ould parse decimal entities', () => {ct(tokenizeAndHumanizeParts('&#65;')).toEqual([[TokenType.TEXT, ''],[TokenType.ENCODED_ENTITY, 'A', '&#65;'],[TokenType.TEXT, ''],[TokenType.EOF],ould store the locations', () => {ct(tokenizeAndHumanizeSourceSpans('a&amp;b')).toEqual([[TokenType.TEXT, 'a'],[TokenType.ENCODED_ENTITY, '&amp;'],[TokenType.TEXT, 'b'],[TokenType.EOF, ''],ould report malformed/unknown entities', () => {ct(tokenizeAndHumanizeErrors('&tbo;')).toEqual([[TokenType.ENCODED_ENTITY,'Unknown entity "tbo" - use the "&#<decimal>;" or  "&#x<hex>;" syntax', '0:0'ct(tokenizeAndHumanizeErrors('&#3sdf;')).toEqual([[TokenType.ENCODED_ENTITY,'Unable to parse entity "&#3s" - decimal character reference entities must end with ";"','0:4'ct(tokenizeAndHumanizeErrors('&#xasdf;')).toEqual([[TokenType.ENCODED_ENTITY,'Unable to parse entity "&#xas" - hexadecimal character reference entities must end with ";"','0:5'ct(tokenizeAndHumanizeErrors('&#xABC')).toEqual([[TokenType.ENCODED_ENTITY, 'Unexpected character "EOF"', '0:6']    });    describe('regular text', () => {ould parse text', () => {ct(tokenizeAndHumanizeParts('a')).toEqual([[TokenType.TEXT, 'a'],[TokenType.EOF],ould parse interpolation', () => {ct(tokenizeAndHumanizeParts(    '{{ a }}b{{ c // comment }}d{{ e "}} \' " f }}g{{ h // " i }}')) .toEqual([   [TokenType.TEXT, ''],   [TokenType.INTERPOLATION, '{{', ' a ', '}}'],   [TokenType.TEXT, 'b'],   [TokenType.INTERPOLATION, '{{', ' c // comment ', '}}'],   [TokenType.TEXT, 'd'],   [TokenType.INTERPOLATION, '{{', ' e "}} \' " f ', '}}'],   [TokenType.TEXT, 'g'],   [TokenType.INTERPOLATION, '{{', ' h // " i ', '}}'],   [TokenType.TEXT, ''],   [TokenType.EOF], ]);ct(tokenizeAndHumanizeSourceSpans('{{ a }}b{{ c // comment }}')).toEqual([[TokenType.TEXT, ''],[TokenType.INTERPOLATION, '{{ a }}'],[TokenType.TEXT, 'b'],[TokenType.INTERPOLATION, '{{ c // comment }}'],[TokenType.TEXT, ''],[TokenType.EOF, ''],ould parse interpolation with custom markers', () => {ct(tokenizeAndHumanizeParts('{% a %}', {interpolationConfig: {start: '{%', end: '%}'}})) .toEqual([   [TokenType.TEXT, ''],   [TokenType.INTERPOLATION, '{%', ' a ', '%}'],   [TokenType.TEXT, ''],   [TokenType.EOF], ]);ould handle CR & LF in text', () => {ct(tokenizeAndHumanizeParts('t\ne\rs\r\nt')).toEqual([[TokenType.TEXT, 't\ne\ns\nt'],[TokenType.EOF],ct(tokenizeAndHumanizeSourceSpans('t\ne\rs\r\nt')).toEqual([[TokenType.TEXT, 't\ne\rs\r\nt'],[TokenType.EOF, ''],ould handle CR & LF in interpolation', () => {ct(tokenizeAndHumanizeParts('{{t\ne\rs\r\nt}}')).toEqual([[TokenType.TEXT, ''],[TokenType.INTERPOLATION, '{{', 't\ne\ns\nt', '}}'],[TokenType.TEXT, ''],[TokenType.EOF],ct(tokenizeAndHumanizeSourceSpans('{{t\ne\rs\r\nt}}')).toEqual([[TokenType.TEXT, ''],[TokenType.INTERPOLATION, '{{t\ne\rs\r\nt}}'],[TokenType.TEXT, ''],[TokenType.EOF, ''],ould parse entities', () => {ct(tokenizeAndHumanizeParts('a&amp;b')).toEqual([[TokenType.TEXT, 'a'],[TokenType.ENCODED_ENTITY, '&', '&amp;'],[TokenType.TEXT, 'b'],[TokenType.EOF],ct(tokenizeAndHumanizeSourceSpans('a&amp;b')).toEqual([[TokenType.TEXT, 'a'],[TokenType.ENCODED_ENTITY, '&amp;'],[TokenType.TEXT, 'b'],[TokenType.EOF, ''],ould parse text starting with "&"', () => {ct(tokenizeAndHumanizeParts('a && b &')).toEqual([[TokenType.TEXT, 'a && b &'],[TokenType.EOF],ould store the locations', () => {ct(tokenizeAndHumanizeSourceSpans('a')).toEqual([[TokenType.TEXT, 'a'],[TokenType.EOF, ''],ould allow "<" in text nodes', () => {ct(tokenizeAndHumanizeParts('{{ a < b ? c : d }}')).toEqual([[TokenType.TEXT, ''],[TokenType.INTERPOLATION, '{{', ' a < b ? c : d ', '}}'],[TokenType.TEXT, ''],[TokenType.EOF],ct(tokenizeAndHumanizeSourceSpans('<p>a<b</p>')).toEqual([[TokenType.TAG_OPEN_START, '<p'],[TokenType.TAG_OPEN_END, '>'],[TokenType.TEXT, 'a'],[TokenType.INCOMPLETE_TAG_OPEN, '<b'],[TokenType.TAG_CLOSE, '</p>'],[TokenType.EOF, ''],ct(tokenizeAndHumanizeParts('< a>')).toEqual([[TokenType.TEXT, '< a>'],[TokenType.EOF],ould break out of interpolation in text token on valid start tag', () => {ct(tokenizeAndHumanizeParts('{{ a <b && c > d }}')).toEqual([[TokenType.TEXT, ''],[TokenType.INTERPOLATION, '{{', ' a '],[TokenType.TEXT, ''],[TokenType.TAG_OPEN_START, '', 'b'],[TokenType.ATTR_NAME, '', '&&'],[TokenType.ATTR_NAME, '', 'c'],[TokenType.TAG_OPEN_END],[TokenType.TEXT, ' d }}'],[TokenType.EOF],ould break out of interpolation in text token on valid comment', () => {ct(tokenizeAndHumanizeParts('{{ a }<!---->}')).toEqual([[TokenType.TEXT, ''],[TokenType.INTERPOLATION, '{{', ' a }'],[TokenType.TEXT, ''],[TokenType.COMMENT_START],[TokenType.RAW_TEXT, ''],[TokenType.COMMENT_END],[TokenType.TEXT, '}'],[TokenType.EOF],ould end interpolation on a valid closing tag', () => {ct(tokenizeAndHumanizeParts('<p>{{ a </p>')).toEqual([[TokenType.TAG_OPEN_START, '', 'p'],[TokenType.TAG_OPEN_END],[TokenType.TEXT, ''],[TokenType.INTERPOLATION, '{{', ' a '],[TokenType.TEXT, ''],[TokenType.TAG_CLOSE, '', 'p'],[TokenType.EOF],ould break out of interpolation in text token on valid CDATA', () => {ct(tokenizeAndHumanizeParts('{{ a }<![CDATA[]]>}')).toEqual([[TokenType.TEXT, ''],[TokenType.INTERPOLATION, '{{', ' a }'],[TokenType.TEXT, ''],[TokenType.CDATA_START],[TokenType.RAW_TEXT, ''],[TokenType.CDATA_END],[TokenType.TEXT, '}'],[TokenType.EOF],ould ignore invalid start tag in interpolation', () => {ote that if the `<=` is considered an "end of text" then the following `{` wouldncorrectly be considered part of an ICU.ct(tokenizeAndHumanizeParts(`<code>{{'<={'}}</code>`, {tokenizeExpansionForms: true})) .toEqual([   [TokenType.TAG_OPEN_START, '', 'code'],   [TokenType.TAG_OPEN_END],   [TokenType.TEXT, ''],   [TokenType.INTERPOLATION, '{{', '\'<={\'', '}}'],   [TokenType.TEXT, ''],   [TokenType.TAG_CLOSE, '', 'code'],   [TokenType.EOF], ]);ould parse start tags quotes in place of an attribute name as text', () => {ct(tokenizeAndHumanizeParts('<t ">')).toEqual([[TokenType.INCOMPLETE_TAG_OPEN, '', 't'],[TokenType.TEXT, '">'],[TokenType.EOF],ct(tokenizeAndHumanizeParts('<t \'>')).toEqual([[TokenType.INCOMPLETE_TAG_OPEN, '', 't'],[TokenType.TEXT, '\'>'],[TokenType.EOF],ould parse start tags quotes in place of an attribute name (after a valid attribute)',expect(tokenizeAndHumanizeParts('<t a="b" ">')).toEqual([  [TokenType.INCOMPLETE_TAG_OPEN, '', 't'],  [TokenType.ATTR_NAME, '', 'a'],  [TokenType.ATTR_QUOTE, '"'],  [TokenType.ATTR_VALUE_TEXT, 'b'],  [TokenType.ATTR_QUOTE, '"'],  // TODO(ayazhafiz): the " symbol should be a synthetic attribute,  // allowing us to complete the opening tag correctly.  [TokenType.TEXT, '">'],  [TokenType.EOF],]);expect(tokenizeAndHumanizeParts('<t a=\'b\' \'>')).toEqual([  [TokenType.INCOMPLETE_TAG_OPEN, '', 't'],  [TokenType.ATTR_NAME, '', 'a'],  [TokenType.ATTR_QUOTE, '\''],  [TokenType.ATTR_VALUE_TEXT, 'b'],  [TokenType.ATTR_QUOTE, '\''],  // TODO(ayazhafiz): the ' symbol should be a synthetic attribute,  // allowing us to complete the opening tag correctly.  [TokenType.TEXT, '\'>'],  [TokenType.EOF],]);ould be able to escape {', () => {ct(tokenizeAndHumanizeParts('{{ "{" }}')).toEqual([[TokenType.TEXT, ''],[TokenType.INTERPOLATION, '{{', ' "{" ', '}}'],[TokenType.TEXT, ''],[TokenType.EOF],ould be able to escape {{', () => {ct(tokenizeAndHumanizeParts('{{ "{{" }}')).toEqual([[TokenType.TEXT, ''],[TokenType.INTERPOLATION, '{{', ' "{{" ', '}}'],[TokenType.TEXT, ''],[TokenType.EOF],ould capture everything up to the end of file in the interpolation expression part if there are mismatched quotes',expect(tokenizeAndHumanizeParts('{{ "{{a}}\' }}')).toEqual([  [TokenType.TEXT, ''],  [TokenType.INTERPOLATION, '{{', ' "{{a}}\' }}'],  [TokenType.TEXT, ''],  [TokenType.EOF],]);ould treat expansion form as text when they are not parsed', () => {ct(tokenizeAndHumanizeParts(    '<span>{a, b, =4 {c}}</span>', {tokenizeExpansionForms: false})) .toEqual([   [TokenType.TAG_OPEN_START, '', 'span'],   [TokenType.TAG_OPEN_END],   [TokenType.TEXT, '{a, b, =4 {c}}'],   [TokenType.TAG_CLOSE, '', 'span'],   [TokenType.EOF], ]);    });    describe('raw text', () => {ould parse text', () => {ct(tokenizeAndHumanizeParts(`<script>t\ne\rs\r\nt</script>`)).toEqual([[TokenType.TAG_OPEN_START, '', 'script'],[TokenType.TAG_OPEN_END],[TokenType.RAW_TEXT, 't\ne\ns\nt'],[TokenType.TAG_CLOSE, '', 'script'],[TokenType.EOF],ould not detect entities', () => {ct(tokenizeAndHumanizeParts(`<script>&amp;</SCRIPT>`)).toEqual([[TokenType.TAG_OPEN_START, '', 'script'],[TokenType.TAG_OPEN_END],[TokenType.RAW_TEXT, '&amp;'],[TokenType.TAG_CLOSE, '', 'script'],[TokenType.EOF],ould ignore other opening tags', () => {ct(tokenizeAndHumanizeParts(`<script>a<div></script>`)).toEqual([[TokenType.TAG_OPEN_START, '', 'script'],[TokenType.TAG_OPEN_END],[TokenType.RAW_TEXT, 'a<div>'],[TokenType.TAG_CLOSE, '', 'script'],[TokenType.EOF],ould ignore other closing tags', () => {ct(tokenizeAndHumanizeParts(`<script>a</test></script>`)).toEqual([[TokenType.TAG_OPEN_START, '', 'script'],[TokenType.TAG_OPEN_END],[TokenType.RAW_TEXT, 'a</test>'],[TokenType.TAG_CLOSE, '', 'script'],[TokenType.EOF],ould store the locations', () => {ct(tokenizeAndHumanizeSourceSpans(`<script>a</script>`)).toEqual([[TokenType.TAG_OPEN_START, '<script'],[TokenType.TAG_OPEN_END, '>'],[TokenType.RAW_TEXT, 'a'],[TokenType.TAG_CLOSE, '</script>'],[TokenType.EOF, ''],    });    describe('escapable raw text', () => {ould parse text', () => {ct(tokenizeAndHumanizeParts(`<title>t\ne\rs\r\nt</title>`)).toEqual([[TokenType.TAG_OPEN_START, '', 'title'],[TokenType.TAG_OPEN_END],[TokenType.ESCAPABLE_RAW_TEXT, 't\ne\ns\nt'],[TokenType.TAG_CLOSE, '', 'title'],[TokenType.EOF],ould detect entities', () => {ct(tokenizeAndHumanizeParts(`<title>&amp;</title>`)).toEqual([[TokenType.TAG_OPEN_START, '', 'title'],[TokenType.TAG_OPEN_END],[TokenType.ESCAPABLE_RAW_TEXT, ''],[TokenType.ENCODED_ENTITY, '&', '&amp;'],[TokenType.ESCAPABLE_RAW_TEXT, ''],[TokenType.TAG_CLOSE, '', 'title'],[TokenType.EOF],ould ignore other opening tags', () => {ct(tokenizeAndHumanizeParts(`<title>a<div></title>`)).toEqual([[TokenType.TAG_OPEN_START, '', 'title'],[TokenType.TAG_OPEN_END],[TokenType.ESCAPABLE_RAW_TEXT, 'a<div>'],[TokenType.TAG_CLOSE, '', 'title'],[TokenType.EOF],ould ignore other closing tags', () => {ct(tokenizeAndHumanizeParts(`<title>a</test></title>`)).toEqual([[TokenType.TAG_OPEN_START, '', 'title'],[TokenType.TAG_OPEN_END],[TokenType.ESCAPABLE_RAW_TEXT, 'a</test>'],[TokenType.TAG_CLOSE, '', 'title'],[TokenType.EOF],ould store the locations', () => {ct(tokenizeAndHumanizeSourceSpans(`<title>a</title>`)).toEqual([[TokenType.TAG_OPEN_START, '<title'],[TokenType.TAG_OPEN_END, '>'],[TokenType.ESCAPABLE_RAW_TEXT, 'a'],[TokenType.TAG_CLOSE, '</title>'],[TokenType.EOF, ''],    });    describe('parsable data', () => {ould parse an SVG <title> tag', () => {ct(tokenizeAndHumanizeParts(`<svg:title>test</svg:title>`)).toEqual([[TokenType.TAG_OPEN_START, 'svg', 'title'],[TokenType.TAG_OPEN_END],[TokenType.TEXT, 'test'],[TokenType.TAG_CLOSE, 'svg', 'title'],[TokenType.EOF],ould parse an SVG <title> tag with children', () => {ct(tokenizeAndHumanizeParts(`<svg:title><f>test</f></svg:title>`)).toEqual([[TokenType.TAG_OPEN_START, 'svg', 'title'],[TokenType.TAG_OPEN_END],[TokenType.TAG_OPEN_START, '', 'f'],[TokenType.TAG_OPEN_END],[TokenType.TEXT, 'test'],[TokenType.TAG_CLOSE, '', 'f'],[TokenType.TAG_CLOSE, 'svg', 'title'],[TokenType.EOF],    });    describe('expansion forms', () => {ould parse an expansion form', () => {ct( tokenizeAndHumanizeParts( '{one.two, three, =4 {four} =5 {five} foo {bar} }', {tokenizeExpansionForms: true})) .toEqual([   [TokenType.EXPANSION_FORM_START],   [TokenType.RAW_TEXT, 'one.two'],   [TokenType.RAW_TEXT, 'three'],   [TokenType.EXPANSION_CASE_VALUE, '=4'],   [TokenType.EXPANSION_CASE_EXP_START],   [TokenType.TEXT, 'four'],   [TokenType.EXPANSION_CASE_EXP_END],   [TokenType.EXPANSION_CASE_VALUE, '=5'],   [TokenType.EXPANSION_CASE_EXP_START],   [TokenType.TEXT, 'five'],   [TokenType.EXPANSION_CASE_EXP_END],   [TokenType.EXPANSION_CASE_VALUE, 'foo'],   [TokenType.EXPANSION_CASE_EXP_START],   [TokenType.TEXT, 'bar'],   [TokenType.EXPANSION_CASE_EXP_END],   [TokenType.EXPANSION_FORM_END],   [TokenType.EOF], ]);ould parse an expansion form with text elements surrounding it', () => {ct(tokenizeAndHumanizeParts(    'before{one.two, three, =4 {four}}after', {tokenizeExpansionForms: true})) .toEqual([   [TokenType.TEXT, 'before'],   [TokenType.EXPANSION_FORM_START],   [TokenType.RAW_TEXT, 'one.two'],   [TokenType.RAW_TEXT, 'three'],   [TokenType.EXPANSION_CASE_VALUE, '=4'],   [TokenType.EXPANSION_CASE_EXP_START],   [TokenType.TEXT, 'four'],   [TokenType.EXPANSION_CASE_EXP_END],   [TokenType.EXPANSION_FORM_END],   [TokenType.TEXT, 'after'],   [TokenType.EOF], ]);ould parse an expansion form as a tag single child', () => {ct(tokenizeAndHumanizeParts(    '<div><span>{a, b, =4 {c}}</span></div>', {tokenizeExpansionForms: true})) .toEqual([   [TokenType.TAG_OPEN_START, '', 'div'],   [TokenType.TAG_OPEN_END],   [TokenType.TAG_OPEN_START, '', 'span'],   [TokenType.TAG_OPEN_END],   [TokenType.EXPANSION_FORM_START],   [TokenType.RAW_TEXT, 'a'],   [TokenType.RAW_TEXT, 'b'],   [TokenType.EXPANSION_CASE_VALUE, '=4'],   [TokenType.EXPANSION_CASE_EXP_START],   [TokenType.TEXT, 'c'],   [TokenType.EXPANSION_CASE_EXP_END],   [TokenType.EXPANSION_FORM_END],   [TokenType.TAG_CLOSE, '', 'span'],   [TokenType.TAG_CLOSE, '', 'div'],   [TokenType.EOF], ]);ould parse an expansion form with whitespace surrounding it', () => {ct(tokenizeAndHumanizeParts(    '<div><span> {a, b, =4 {c}} </span></div>', {tokenizeExpansionForms: true})) .toEqual([   [TokenType.TAG_OPEN_START, '', 'div'],   [TokenType.TAG_OPEN_END],   [TokenType.TAG_OPEN_START, '', 'span'],   [TokenType.TAG_OPEN_END],   [TokenType.TEXT, ' '],   [TokenType.EXPANSION_FORM_START],   [TokenType.RAW_TEXT, 'a'],   [TokenType.RAW_TEXT, 'b'],   [TokenType.EXPANSION_CASE_VALUE, '=4'],   [TokenType.EXPANSION_CASE_EXP_START],   [TokenType.TEXT, 'c'],   [TokenType.EXPANSION_CASE_EXP_END],   [TokenType.EXPANSION_FORM_END],   [TokenType.TEXT, ' '],   [TokenType.TAG_CLOSE, '', 'span'],   [TokenType.TAG_CLOSE, '', 'div'],   [TokenType.EOF], ]);ould parse an expansion forms with elements in it', () => {ct(tokenizeAndHumanizeParts(    '{one.two, three, =4 {four <b>a</b>}}', {tokenizeExpansionForms: true})) .toEqual([   [TokenType.EXPANSION_FORM_START],   [TokenType.RAW_TEXT, 'one.two'],   [TokenType.RAW_TEXT, 'three'],   [TokenType.EXPANSION_CASE_VALUE, '=4'],   [TokenType.EXPANSION_CASE_EXP_START],   [TokenType.TEXT, 'four '],   [TokenType.TAG_OPEN_START, '', 'b'],   [TokenType.TAG_OPEN_END],   [TokenType.TEXT, 'a'],   [TokenType.TAG_CLOSE, '', 'b'],   [TokenType.EXPANSION_CASE_EXP_END],   [TokenType.EXPANSION_FORM_END],   [TokenType.EOF], ]);ould parse an expansion forms containing an interpolation', () => {ct(tokenizeAndHumanizeParts(    '{one.two, three, =4 {four {{a}}}}', {tokenizeExpansionForms: true})) .toEqual([   [TokenType.EXPANSION_FORM_START],   [TokenType.RAW_TEXT, 'one.two'],   [TokenType.RAW_TEXT, 'three'],   [TokenType.EXPANSION_CASE_VALUE, '=4'],   [TokenType.EXPANSION_CASE_EXP_START],   [TokenType.TEXT, 'four '],   [TokenType.INTERPOLATION, '{{', 'a', '}}'],   [TokenType.TEXT, ''],   [TokenType.EXPANSION_CASE_EXP_END],   [TokenType.EXPANSION_FORM_END],   [TokenType.EOF], ]);ould parse nested expansion forms', () => {ct(tokenizeAndHumanizeParts(    `{one.two, three, =4 { {xx, yy, =x {one}} }}`, {tokenizeExpansionForms: true})) .toEqual([   [TokenType.EXPANSION_FORM_START],   [TokenType.RAW_TEXT, 'one.two'],   [TokenType.RAW_TEXT, 'three'],   [TokenType.EXPANSION_CASE_VALUE, '=4'],   [TokenType.EXPANSION_CASE_EXP_START],   [TokenType.EXPANSION_FORM_START],   [TokenType.RAW_TEXT, 'xx'],   [TokenType.RAW_TEXT, 'yy'],   [TokenType.EXPANSION_CASE_VALUE, '=x'],   [TokenType.EXPANSION_CASE_EXP_START],   [TokenType.TEXT, 'one'],   [TokenType.EXPANSION_CASE_EXP_END],   [TokenType.EXPANSION_FORM_END],   [TokenType.TEXT, ' '],   [TokenType.EXPANSION_CASE_EXP_END],   [TokenType.EXPANSION_FORM_END],   [TokenType.EOF], ]);be('[line ending normalization', () => {ribe('{escapedString: true}', () => {it('should normalize line-endings in expansion forms if `i18nNormalizeLineEndingsInICUs` is true',  () => {const result = tokenizeWithoutErrors(    `{\r\n` + messages.length,\r\n` + plural,\r\n` + =0 {You have \r\nno\r\n messages}\r\n` + =1 {One {{message}}}}\r\n`,    {zeExpansionForms: true,dString: true,rmalizeLineEndingsInICUs: true    });expect(humanizeParts(result.tokens)).toEqual([  [TokenType.EXPANSION_FORM_START],  [TokenType.RAW_TEXT, '\n    messages.length'],  [TokenType.RAW_TEXT, 'plural'],  [TokenType.EXPANSION_CASE_VALUE, '=0'],  [TokenType.EXPANSION_CASE_EXP_START],  [TokenType.TEXT, 'You have \nno\n messages'],  [TokenType.EXPANSION_CASE_EXP_END],  [TokenType.EXPANSION_CASE_VALUE, '=1'],  [TokenType.EXPANSION_CASE_EXP_START],  [TokenType.TEXT, 'One '],  [TokenType.INTERPOLATION, '{{', 'message', '}}'],  [TokenType.TEXT, ''],  [TokenType.EXPANSION_CASE_EXP_END],  [TokenType.EXPANSION_FORM_END],  [TokenType.TEXT, '\n'],  [TokenType.EOF],]);expect(result.nonNormalizedIcuExpressions).toEqual([]);  });it('should not normalize line-endings in ICU expressions when `i18nNormalizeLineEndingsInICUs` is not defined',  () => {const result = tokenizeWithoutErrors(    `{\r\n` + messages.length,\r\n` + plural,\r\n` + =0 {You have \r\nno\r\n messages}\r\n` + =1 {One {{message}}}}\r\n`,    {tokenizeExpansionForms: true, escapedString: true});expect(humanizeParts(result.tokens)).toEqual([  [TokenType.EXPANSION_FORM_START],  [TokenType.RAW_TEXT, '\r\n    messages.length'],  [TokenType.RAW_TEXT, 'plural'],  [TokenType.EXPANSION_CASE_VALUE, '=0'],  [TokenType.EXPANSION_CASE_EXP_START],  [TokenType.TEXT, 'You have \nno\n messages'],  [TokenType.EXPANSION_CASE_EXP_END],  [TokenType.EXPANSION_CASE_VALUE, '=1'],  [TokenType.EXPANSION_CASE_EXP_START],  [TokenType.TEXT, 'One '],  [TokenType.INTERPOLATION, '{{', 'message', '}}'],  [TokenType.TEXT, ''],  [TokenType.EXPANSION_CASE_EXP_END],  [TokenType.EXPANSION_FORM_END],  [TokenType.TEXT, '\n'],  [TokenType.EOF],]);expect(result.nonNormalizedIcuExpressions!.length).toBe(1);expect(result.nonNormalizedIcuExpressions![0].sourceSpan.toString())    .toEqual('\r\n    messages.length');  });it('should not normalize line endings in nested expansion forms when `i18nNormalizeLineEndingsInICUs` is not defined',  () => {const result = tokenizeWithoutErrors(    `{\r\n` +essages.length, plural,\r\n` +0 { zero \r\n` +r\nlect,\r\n` +` +r\n  }\r\n` +    {tokenizeExpansionForms: true, escapedString: true});expect(humanizeParts(result.tokens)).toEqual([  [TokenType.EXPANSION_FORM_START],  [TokenType.RAW_TEXT, '\r\n  messages.length'],  [TokenType.RAW_TEXT, 'plural'],  [TokenType.EXPANSION_CASE_VALUE, '=0'],  [TokenType.EXPANSION_CASE_EXP_START],  [TokenType.TEXT, 'zero \n  [TokenType.EXPANSION_FORM_START],  [TokenType.RAW_TEXT, '\r\n],  [TokenType.RAW_TEXT, 'select'],  [TokenType.EXPANSION_CASE_VALUE, 'male'],  [TokenType.EXPANSION_CASE_EXP_START],  [TokenType.TEXT, 'm'],  [TokenType.EXPANSION_CASE_EXP_END],  [TokenType.EXPANSION_FORM_END],  [TokenType.TEXT, '\n     '],  [TokenType.EXPANSION_CASE_EXP_END],  [TokenType.EXPANSION_FORM_END],  [TokenType.EOF],]);expect(result.nonNormalizedIcuExpressions!.length).toBe(2);expect(result.nonNormalizedIcuExpressions![0].sourceSpan.toString())    .toEqual('\r\n  messages.length');expect(result.nonNormalizedIcuExpressions![1].sourceSpan.toString())    .toEqual('\r\n);  });ribe('{escapedString: false}', () => {it('should normalize line-endings in expansion forms if `i18nNormalizeLineEndingsInICUs` is true',  () => {const result = tokenizeWithoutErrors(    `{\r\n` + messages.length,\r\n` + plural,\r\n` + =0 {You have \r\nno\r\n messages}\r\n` + =1 {One {{message}}}}\r\n`,    {zeExpansionForms: true,dString: false,rmalizeLineEndingsInICUs: true    });expect(humanizeParts(result.tokens)).toEqual([  [TokenType.EXPANSION_FORM_START],  [TokenType.RAW_TEXT, '\n    messages.length'],  [TokenType.RAW_TEXT, 'plural'],  [TokenType.EXPANSION_CASE_VALUE, '=0'],  [TokenType.EXPANSION_CASE_EXP_START],  [TokenType.TEXT, 'You have \nno\n messages'],  [TokenType.EXPANSION_CASE_EXP_END],  [TokenType.EXPANSION_CASE_VALUE, '=1'],  [TokenType.EXPANSION_CASE_EXP_START],  [TokenType.TEXT, 'One '],  [TokenType.INTERPOLATION, '{{', 'message', '}}'],  [TokenType.TEXT, ''],  [TokenType.EXPANSION_CASE_EXP_END],  [TokenType.EXPANSION_FORM_END],  [TokenType.TEXT, '\n'],  [TokenType.EOF],]);expect(result.nonNormalizedIcuExpressions).toEqual([]);  });it('should not normalize line-endings in ICU expressions when `i18nNormalizeLineEndingsInICUs` is not defined',  () => {const result = tokenizeWithoutErrors(    `{\r\n` + messages.length,\r\n` + plural,\r\n` + =0 {You have \r\nno\r\n messages}\r\n` + =1 {One {{message}}}}\r\n`,    {tokenizeExpansionForms: true, escapedString: false});expect(humanizeParts(result.tokens)).toEqual([  [TokenType.EXPANSION_FORM_START],  [TokenType.RAW_TEXT, '\r\n    messages.length'],  [TokenType.RAW_TEXT, 'plural'],  [TokenType.EXPANSION_CASE_VALUE, '=0'],  [TokenType.EXPANSION_CASE_EXP_START],  [TokenType.TEXT, 'You have \nno\n messages'],  [TokenType.EXPANSION_CASE_EXP_END],  [TokenType.EXPANSION_CASE_VALUE, '=1'],  [TokenType.EXPANSION_CASE_EXP_START],  [TokenType.TEXT, 'One '],  [TokenType.INTERPOLATION, '{{', 'message', '}}'],  [TokenType.TEXT, ''],  [TokenType.EXPANSION_CASE_EXP_END],  [TokenType.EXPANSION_FORM_END],  [TokenType.TEXT, '\n'],  [TokenType.EOF],]);expect(result.nonNormalizedIcuExpressions!.length).toBe(1);expect(result.nonNormalizedIcuExpressions![0].sourceSpan.toString())    .toEqual('\r\n    messages.length');  });it('should not normalize line endings in nested expansion forms when `i18nNormalizeLineEndingsInICUs` is not defined',  () => {const result = tokenizeWithoutErrors(    `{\r\n` +essages.length, plural,\r\n` +0 { zero \r\n` +r\nlect,\r\n` +` +r\n  }\r\n` +    {tokenizeExpansionForms: true});expect(humanizeParts(result.tokens)).toEqual([  [TokenType.EXPANSION_FORM_START],  [TokenType.RAW_TEXT, '\r\n  messages.length'],  [TokenType.RAW_TEXT, 'plural'],  [TokenType.EXPANSION_CASE_VALUE, '=0'],  [TokenType.EXPANSION_CASE_EXP_START],  [TokenType.TEXT, 'zero \n  [TokenType.EXPANSION_FORM_START],  [TokenType.RAW_TEXT, '\r\n],  [TokenType.RAW_TEXT, 'select'],  [TokenType.EXPANSION_CASE_VALUE, 'male'],  [TokenType.EXPANSION_CASE_EXP_START],  [TokenType.TEXT, 'm'],  [TokenType.EXPANSION_CASE_EXP_END],  [TokenType.EXPANSION_FORM_END],  [TokenType.TEXT, '\n     '],  [TokenType.EXPANSION_CASE_EXP_END],  [TokenType.EXPANSION_FORM_END],  [TokenType.EOF],]);expect(result.nonNormalizedIcuExpressions!.length).toBe(2);expect(result.nonNormalizedIcuExpressions![0].sourceSpan.toString())    .toEqual('\r\n  messages.length');expect(result.nonNormalizedIcuExpressions![1].sourceSpan.toString())    .toEqual('\r\n);  });    });    describe('errors', () => {ould report unescaped "{" on error', () => {ct(tokenizeAndHumanizeErrors(`<p>before { after</p>`, {tokenizeExpansionForms: true})) .toEqual([[   TokenType.RAW_TEXT,   `Unexpected character "EOF" (Do you have an unescaped "{" in your template? Use "{{ '{' }}") to escape it.)`,   '0:21', ]]);ould report unescaped "{" as an error, even after a prematurely terminated interpolation',expect(tokenizeAndHumanizeErrors(e>{{b}<!---->}</code><pre>import {a} from 'a';</pre>`,nizeExpansionForms: true})).toEqual([[  TokenType.RAW_TEXT,  `Unexpected character "EOF" (Do you have an unescaped "{" in your template? Use "{{ '{' }}") to escape it.)`,  '0:56',]]);ould include 2 lines of context in message', () => {t src = '111\n222\n333\nE\n444\n555\n666\n';t file = new ParseSourceFile(src, 'file://');t location = new ParseLocation(file, 12, 123, 456);t span = new ParseSourceSpan(location, location);t error = new TokenError('**ERROR**', null!, span);ct(error.toString()) .toEqual(`**ERROR** ("\n222\n333\n[ERROR ->]E\n444\n555\n"): file://@123:456`);    });    describe('unicode characters', () => {ould support unicode characters', () => {ct(tokenizeAndHumanizeSourceSpans(`<p>İ</p>`)).toEqual([[TokenType.TAG_OPEN_START, '<p'],[TokenType.TAG_OPEN_END, '>'],[TokenType.TEXT, 'İ'],[TokenType.TAG_CLOSE, '</p>'],[TokenType.EOF, ''],    });    describe('(processing escaped strings)', () => {ould unescape standard escape sequences', () => {ct(tokenizeAndHumanizeParts('\\\' \\\' \\\'', {escapedString: true})).toEqual([[TokenType.TEXT, '\' \' \''],[TokenType.EOF],ct(tokenizeAndHumanizeParts('\\" \\" \\"', {escapedString: true})).toEqual([[TokenType.TEXT, '\" \" \"'],[TokenType.EOF],ct(tokenizeAndHumanizeParts('\\` \\` \\`', {escapedString: true})).toEqual([[TokenType.TEXT, '\` \` \`'],[TokenType.EOF],ct(tokenizeAndHumanizeParts('\\\\ \\\\ \\\\', {escapedString: true})).toEqual([[TokenType.TEXT, '\\ \\ \\'],[TokenType.EOF],ct(tokenizeAndHumanizeParts('\\n \\n \\n', {escapedString: true})).toEqual([[TokenType.TEXT, '\n \n \n'],[TokenType.EOF],ct(tokenizeAndHumanizeParts('\\r{{\\r}}\\r', {escapedString: true})).toEqual([// post processing converts `\r` to `\n`[TokenType.TEXT, '\n'],[TokenType.INTERPOLATION, '{{', '\n', '}}'],[TokenType.TEXT, '\n'],[TokenType.EOF],ct(tokenizeAndHumanizeParts('\\v \\v \\v', {escapedString: true})).toEqual([[TokenType.TEXT, '\v \v \v'],[TokenType.EOF],ct(tokenizeAndHumanizeParts('\\t \\t \\t', {escapedString: true})).toEqual([[TokenType.TEXT, '\t \t \t'],[TokenType.EOF],ct(tokenizeAndHumanizeParts('\\b \\b \\b', {escapedString: true})).toEqual([[TokenType.TEXT, '\b \b \b'],[TokenType.EOF],ct(tokenizeAndHumanizeParts('\\f \\f \\f', {escapedString: true})).toEqual([[TokenType.TEXT, '\f \f \f'],[TokenType.EOF],ct(tokenizeAndHumanizeParts(    '\\\' \\" \\` \\\\ \\n \\r \\v \\t \\b \\f', {escapedString: true})) .toEqual([   [TokenType.TEXT, '\' \" \` \\ \n \n \v \t \b \f'],   [TokenType.EOF], ]);ould unescape null sequences', () => {ct(tokenizeAndHumanizeParts('\\0', {escapedString: true})).toEqual([[TokenType.EOF],09 is not an octal number so the \0 is taken as EOFct(tokenizeAndHumanizeParts('\\09', {escapedString: true})).toEqual([[TokenType.EOF],ould unescape octal sequences', () => {19 is read as an octal `\1` followed by a normal char `9`1234 is read as an octal `\123` followed by a normal char `4`999 is not an octal number so its backslash just gets removed.ct(tokenizeAndHumanizeParts(    '\\001 \\01 \\1 \\12 \\223 \\19 \\2234 \\999', {escapedString: true})) .toEqual([   [TokenType.TEXT, '\x01 \x01 \x01 \x0A \x93 \x019 \x934 999'],   [TokenType.EOF], ]);ould unescape hex sequences', () => {ct(tokenizeAndHumanizeParts('\\x12 \\x4F \\xDC', {escapedString: true})).toEqual([[TokenType.TEXT, '\x12 \x4F \xDC'],[TokenType.EOF],ould report an error on an invalid hex sequence', () => {ct(tokenizeAndHumanizeErrors('\\xGG', {escapedString: true})).toEqual([[null, 'Invalid hexadecimal escape sequence', '0:2']ct(tokenizeAndHumanizeErrors('abc \\x xyz', {escapedString: true})).toEqual([[TokenType.TEXT, 'Invalid hexadecimal escape sequence', '0:6']ct(tokenizeAndHumanizeErrors('abc\\x', {escapedString: true})).toEqual([[TokenType.TEXT, 'Unexpected character "EOF"', '0:5']ould unescape fixed length Unicode sequences', () => {ct(tokenizeAndHumanizeParts('\\u0123 \\uABCD', {escapedString: true})).toEqual([[TokenType.TEXT, '\u0123 \uABCD'],[TokenType.EOF],ould error on an invalid fixed length Unicode sequence', () => {ct(tokenizeAndHumanizeErrors('\\uGGGG', {escapedString: true})).toEqual([[null, 'Invalid hexadecimal escape sequence', '0:2']ould unescape variable length Unicode sequences', () => {ct(tokenizeAndHumanizeParts(    '\\u{01} \\u{ABC} \\u{1234} \\u{123AB}', {escapedString: true})) .toEqual([   [TokenType.TEXT, '\u{01} \u{ABC} \u{1234} \u{123AB}'],   [TokenType.EOF], ]);ould error on an invalid variable length Unicode sequence', () => {ct(tokenizeAndHumanizeErrors('\\u{GG}', {escapedString: true})).toEqual([[null, 'Invalid hexadecimal escape sequence', '0:3']ould unescape line continuations', () => {ct(tokenizeAndHumanizeParts('abc\\\ndef', {escapedString: true})).toEqual([[TokenType.TEXT, 'abcdef'],[TokenType.EOF],ct(tokenizeAndHumanizeParts('\\\nx\\\ny\\\n', {escapedString: true})).toEqual([[TokenType.TEXT, 'xy'],[TokenType.EOF],ould remove backslash from "non-escape" sequences', () => {ct(tokenizeAndHumanizeParts('\a \g \~', {escapedString: true})).toEqual([[TokenType.TEXT, 'a g ~'],[TokenType.EOF],ould unescape sequences in plain text', () => {ct(tokenizeAndHumanizeParts('abc\ndef\\nghi\\tjkl\\`\\\'\\"mno', {escapedString: true})) .toEqual([   [TokenType.TEXT, 'abc\ndef\nghi\tjkl`\'"mno'],   [TokenType.EOF], ]);ould unescape sequences in raw text', () => {ct(tokenizeAndHumanizeParts(    '<script>abc\ndef\\nghi\\tjkl\\`\\\'\\"mno</script>', {escapedString: true})) .toEqual([   [TokenType.TAG_OPEN_START, '', 'script'],   [TokenType.TAG_OPEN_END],   [TokenType.RAW_TEXT, 'abc\ndef\nghi\tjkl`\'"mno'],   [TokenType.TAG_CLOSE, '', 'script'],   [TokenType.EOF], ]);ould unescape sequences in escapable raw text', () => {ct(tokenizeAndHumanizeParts(    '<title>abc\ndef\\nghi\\tjkl\\`\\\'\\"mno</title>', {escapedString: true})) .toEqual([   [TokenType.TAG_OPEN_START, '', 'title'],   [TokenType.TAG_OPEN_END],   [TokenType.ESCAPABLE_RAW_TEXT, 'abc\ndef\nghi\tjkl`\'"mno'],   [TokenType.TAG_CLOSE, '', 'title'],   [TokenType.EOF], ]);ould parse over escape sequences in tag definitions', () => {ct(tokenizeAndHumanizeParts('<t a=\\"b\\" \\n c=\\\'d\\\'>', {escapedString: true})) .toEqual([   [TokenType.TAG_OPEN_START, '', 't'],   [TokenType.ATTR_NAME, '', 'a'],   [TokenType.ATTR_QUOTE, '"'],   [TokenType.ATTR_VALUE_TEXT, 'b'],   [TokenType.ATTR_QUOTE, '"'],   [TokenType.ATTR_NAME, '', 'c'],   [TokenType.ATTR_QUOTE, '\''],   [TokenType.ATTR_VALUE_TEXT, 'd'],   [TokenType.ATTR_QUOTE, '\''],   [TokenType.TAG_OPEN_END],   [TokenType.EOF], ]);ould parse over escaped new line in tag definitions', () => {t text = '<t\\n></t>';ct(tokenizeAndHumanizeParts(text, {escapedString: true})).toEqual([[TokenType.TAG_OPEN_START, '', 't'],[TokenType.TAG_OPEN_END],[TokenType.TAG_CLOSE, '', 't'],[TokenType.EOF],ould parse over escaped characters in tag definitions', () => {t text = '<t\u{000013}></t>';ct(tokenizeAndHumanizeParts(text, {escapedString: true})).toEqual([[TokenType.TAG_OPEN_START, '', 't'],[TokenType.TAG_OPEN_END],[TokenType.TAG_CLOSE, '', 't'],[TokenType.EOF],ould unescape characters in tag names', () => {t text = '<t\\x64></t\\x64>';ct(tokenizeAndHumanizeParts(text, {escapedString: true})).toEqual([[TokenType.TAG_OPEN_START, '', 'td'],[TokenType.TAG_OPEN_END],[TokenType.TAG_CLOSE, '', 'td'],[TokenType.EOF],ct(tokenizeAndHumanizeSourceSpans(text, {escapedString: true})).toEqual([[TokenType.TAG_OPEN_START, '<t\\x64'],[TokenType.TAG_OPEN_END, '>'],[TokenType.TAG_CLOSE, '</t\\x64>'],[TokenType.EOF, ''],ould unescape characters in attributes', () => {t text = '<t \\x64="\\x65"></t>';ct(tokenizeAndHumanizeParts(text, {escapedString: true})).toEqual([[TokenType.TAG_OPEN_START, '', 't'],[TokenType.ATTR_NAME, '', 'd'],[TokenType.ATTR_QUOTE, '"'],[TokenType.ATTR_VALUE_TEXT, 'e'],[TokenType.ATTR_QUOTE, '"'],[TokenType.TAG_OPEN_END],[TokenType.TAG_CLOSE, '', 't'],[TokenType.EOF],ould parse over escaped new line in attribute values', () => {t text = '<t a=b\\n></t>';ct(tokenizeAndHumanizeParts(text, {escapedString: true})).toEqual([[TokenType.TAG_OPEN_START, '', 't'],[TokenType.ATTR_NAME, '', 'a'],[TokenType.ATTR_VALUE_TEXT, 'b'],[TokenType.TAG_OPEN_END],[TokenType.TAG_CLOSE, '', 't'],[TokenType.EOF],ould tokenize the correct span when there are escape sequences', () => {t text = 'selector: "app-root",\ntemplate: "line 1\\n\\"line 2\\"\\nline 3",\ninputs: []';t range = {startPos: 33,startLine: 1,startCol: 10,endPos: 59,ct(tokenizeAndHumanizeParts(text, {range, escapedString: true})).toEqual([[TokenType.TEXT, 'line 1\n"line 2"\nline 3'],[TokenType.EOF],ct(tokenizeAndHumanizeSourceSpans(text, {range, escapedString: true})).toEqual([[TokenType.TEXT, 'line 1\\n\\"line 2\\"\\nline 3'],[TokenType.EOF, ''],ould account for escape sequences when computing source spans ', () => {t text = '<t>line 1</t>\n' +  // <- unescaped line break '<t>line 2</t>\\n' +// <- escaped line break '<t>line 3\\\n' +  // <- line continuation '</t>';ct(tokenizeAndHumanizeParts(text, {escapedString: true})).toEqual([[TokenType.TAG_OPEN_START, '', 't'], [TokenType.TAG_OPEN_END], [TokenType.TEXT, 'line 1'],[TokenType.TAG_CLOSE, '', 't'], [TokenType.TEXT, '\n'],[TokenType.TAG_OPEN_START, '', 't'], [TokenType.TAG_OPEN_END], [TokenType.TEXT, 'line 2'],[TokenType.TAG_CLOSE, '', 't'], [TokenType.TEXT, '\n'],[TokenType.TAG_OPEN_START, '', 't'], [TokenType.TAG_OPEN_END],[TokenType.TEXT, 'line 3'],  // <- line continuation does not appear in token[TokenType.TAG_CLOSE, '', 't'],[TokenType.EOF]ct(tokenizeAndHumanizeLineColumn(text, {escapedString: true})).toEqual([[TokenType.TAG_OPEN_START, '0:0'],[TokenType.TAG_OPEN_END, '0:2'],[TokenType.TEXT, '0:3'],[TokenType.TAG_CLOSE, '0:9'],[TokenType.TEXT, '0:13'],  // <- real newline increments the row[TokenType.TAG_OPEN_START, '1:0'],[TokenType.TAG_OPEN_END, '1:2'],[TokenType.TEXT, '1:3'],[TokenType.TAG_CLOSE, '1:9'],[TokenType.TEXT, '1:13'],  // <- escaped newline does not increment the row[TokenType.TAG_OPEN_START, '1:15'],[TokenType.TAG_OPEN_END, '1:17'],[TokenType.TEXT, '1:18'],  // <- the line continuation increments the row[TokenType.TAG_CLOSE, '2:0'],[TokenType.EOF, '2:4'],ct(tokenizeAndHumanizeSourceSpans(text, {escapedString: true})).toEqual([[TokenType.TAG_OPEN_START, '<t'], [TokenType.TAG_OPEN_END, '>'],[TokenType.TEXT, 'line 1'], [TokenType.TAG_CLOSE, '</t>'], [TokenType.TEXT, '\n'],[TokenType.TAG_OPEN_START, '<t'], [TokenType.TAG_OPEN_END, '>'],[TokenType.TEXT, 'line 2'], [TokenType.TAG_CLOSE, '</t>'], [TokenType.TEXT, '\\n'],[TokenType.TAG_OPEN_START, '<t'], [TokenType.TAG_OPEN_END, '>'],[TokenType.TEXT, 'line 3\\\n'], [TokenType.TAG_CLOSE, '</t>'],[TokenType.EOF, '']    });    describe('blocks', () => {O(crisbeto): temporary utility while blocks are disabled by default.options: Readonly<TokenizeOptions> = {tokenizeBlocks: true};on tokenizeBlock(input: string, additionalOptions: TokenizeOptions = {}): any[] {rn tokenizeAndHumanizeParts(input, {...options, ...additionalOptions});ould parse a block group', () => {ct(tokenizeBlock('{#foo}hello{/foo}')).toEqual([[TokenType.BLOCK_GROUP_OPEN_START, 'foo'],[TokenType.BLOCK_GROUP_OPEN_END],[TokenType.TEXT, 'hello'],[TokenType.BLOCK_GROUP_CLOSE, 'foo'],[TokenType.EOF],ould parse a block group with parameters', () => {ct(tokenizeBlock('{#for item of items; track item.id}hello{/for}')).toEqual([[TokenType.BLOCK_GROUP_OPEN_START, 'for'],[TokenType.BLOCK_PARAMETER, 'item of items'],[TokenType.BLOCK_PARAMETER, 'track item.id'],[TokenType.BLOCK_GROUP_OPEN_END],[TokenType.TEXT, 'hello'],[TokenType.BLOCK_GROUP_CLOSE, 'for'],[TokenType.EOF],ould parse a block group with a trailing semicolon after the parameters', () => {ct(tokenizeBlock('{#for item of items;}hello{/for}')).toEqual([[TokenType.BLOCK_GROUP_OPEN_START, 'for'],[TokenType.BLOCK_PARAMETER, 'item of items'],[TokenType.BLOCK_GROUP_OPEN_END],[TokenType.TEXT, 'hello'],[TokenType.BLOCK_GROUP_CLOSE, 'for'],[TokenType.EOF],ould parse a block group with multiple trailing semicolons', () => {ct(tokenizeBlock('{#for item of items;;;;;}hello{/for}')).toEqual([[TokenType.BLOCK_GROUP_OPEN_START, 'for'],[TokenType.BLOCK_PARAMETER, 'item of items'],[TokenType.BLOCK_GROUP_OPEN_END],[TokenType.TEXT, 'hello'],[TokenType.BLOCK_GROUP_CLOSE, 'for'],[TokenType.EOF],ould parse a block group with trailing whitespace', () => {ct(tokenizeBlock('{#foooo}')).toEqual([[TokenType.BLOCK_GROUP_OPEN_START, 'foo'],[TokenType.BLOCK_GROUP_OPEN_END],[TokenType.TEXT, 'hello'],[TokenType.BLOCK_GROUP_CLOSE, 'foo'],[TokenType.EOF],ould parse a block group with no trailing semicolon', () => {ct(tokenizeBlock('{#for item of items}hello{/for}')).toEqual([[TokenType.BLOCK_GROUP_OPEN_START, 'for'],[TokenType.BLOCK_PARAMETER, 'item of items'],[TokenType.BLOCK_GROUP_OPEN_END],[TokenType.TEXT, 'hello'],[TokenType.BLOCK_GROUP_CLOSE, 'for'],[TokenType.EOF],ould handle semicolons and braces used in a block group parameter', () => {ct(tokenizeBlock(`{#foo a === ";"; b === '}'; c === "{"}hello{/foo}`)).toEqual([[TokenType.BLOCK_GROUP_OPEN_START, 'foo'],[TokenType.BLOCK_PARAMETER, `a === ";"`],[TokenType.BLOCK_PARAMETER, `b === '}'`],[TokenType.BLOCK_PARAMETER, `c === "{"`],[TokenType.BLOCK_GROUP_OPEN_END],[TokenType.TEXT, 'hello'],[TokenType.BLOCK_GROUP_CLOSE, 'foo'],[TokenType.EOF],ould handle object literals in block group parameters', () => {ct(tokenizeBlock(`{#foo on a({a: 1, b: 2}, false, {c: 3}); when b({d: 4})}hello{/foo}`)) .toEqual([   [TokenType.BLOCK_GROUP_OPEN_START, 'foo'],   [TokenType.BLOCK_PARAMETER, 'on a({a: 1, b: 2}, false, {c: 3})'],   [TokenType.BLOCK_PARAMETER, 'when b({d: 4})'],   [TokenType.BLOCK_GROUP_OPEN_END],   [TokenType.TEXT, 'hello'],   [TokenType.BLOCK_GROUP_CLOSE, 'foo'],   [TokenType.EOF], ]);ould report invalid quotes in a parameter', () => {ct(tokenizeAndHumanizeErrors(`{#foo a === "}hello{/foo}`, options)).toEqual([[TokenType.BLOCK_PARAMETER, 'Unexpected character "EOF"', '0:25']ct(tokenizeAndHumanizeErrors(`{#foo a === "hi'}hello{/foo}`, options)).toEqual([[TokenType.BLOCK_PARAMETER, 'Unexpected character "EOF"', '0:28']ould report unclosed block open tag containing an objet literal', () => {ct(tokenizeAndHumanizeErrors(`{#foo {invalid: true}hello{/foo}`, options)).toEqual([[TokenType.BLOCK_GROUP_OPEN_END, 'Unexpected character "EOF"', '0:32'],ould report unclosed object literal in block open tag', () => {ct(tokenizeAndHumanizeErrors(`{#foo {hello{/foo}`, options)).toEqual([[TokenType.BLOCK_GROUP_OPEN_END, 'Unexpected character "EOF"', '0:18'],ould handle a semicolon used in a nested string inside a block group parameter', () => {ct(tokenizeBlock(`{#if condition === "';'"}hello{/if}`)).toEqual([[TokenType.BLOCK_GROUP_OPEN_START, 'if'],[TokenType.BLOCK_PARAMETER, `condition === "';'"`],[TokenType.BLOCK_GROUP_OPEN_END],[TokenType.TEXT, 'hello'],[TokenType.BLOCK_GROUP_CLOSE, 'if'],[TokenType.EOF],ould handle a semicolon next to an escaped quote used in a block group parameter',expect(tokenizeBlock('{#if condition === "\\";"}hello{/if}')).toEqual([  [TokenType.BLOCK_GROUP_OPEN_START, 'if'],  [TokenType.BLOCK_PARAMETER, 'condition === "\\";"'],  [TokenType.BLOCK_GROUP_OPEN_END],  [TokenType.TEXT, 'hello'],  [TokenType.BLOCK_GROUP_CLOSE, 'if'],  [TokenType.EOF],]);ould parse mixed text and html content in a block group', () => {ct(tokenizeBlock('{#if a === 1}foo <b>bar</b> baz{/if}')).toEqual([[TokenType.BLOCK_GROUP_OPEN_START, 'if'],[TokenType.BLOCK_PARAMETER, 'a === 1'],[TokenType.BLOCK_GROUP_OPEN_END],[TokenType.TEXT, 'foo '],[TokenType.TAG_OPEN_START, '', 'b'],[TokenType.TAG_OPEN_END],[TokenType.TEXT, 'bar'],[TokenType.TAG_CLOSE, '', 'b'],[TokenType.TEXT, ' baz'],[TokenType.BLOCK_GROUP_CLOSE, 'if'],[TokenType.EOF],ould parse a block group with blocks', () => {ct(tokenizeBlock(    '{#if expr}' +    'foo' +    '{:else if otherExpr === 1}' +    '<some-comp/>' +    '{:else}' +    'bar' +    '{/if}')) .toEqual([   [TokenType.BLOCK_GROUP_OPEN_START, 'if'],   [TokenType.BLOCK_PARAMETER, 'expr'],   [TokenType.BLOCK_GROUP_OPEN_END],   [TokenType.TEXT, 'foo'],   [TokenType.BLOCK_OPEN_START, 'else'],   [TokenType.BLOCK_PARAMETER, 'if otherExpr === 1'],   [TokenType.BLOCK_OPEN_END],   [TokenType.TAG_OPEN_START, '', 'some-comp'],   [TokenType.TAG_OPEN_END_VOID],   [TokenType.BLOCK_OPEN_START, 'else'],   [TokenType.BLOCK_OPEN_END],   [TokenType.TEXT, 'bar'],   [TokenType.BLOCK_GROUP_CLOSE, 'if'],   [TokenType.EOF], ]);ould parse nested block groups', () => {ct(tokenizeBlock(    '{#if a}' +    'hello a' +    '{:else}' +    '{#if b}' +    'hello b' +    '{:else}' +    '{#if c}hello c{/if}' +    '{/if}' +    '{/if}')) .toEqual([   [TokenType.BLOCK_GROUP_OPEN_START, 'if'], [TokenType.BLOCK_PARAMETER, 'a'],   [TokenType.BLOCK_GROUP_OPEN_END],e.TEXT, 'hello a'],   [TokenType.BLOCK_OPEN_START, 'else'],     [TokenType.BLOCK_OPEN_END],   [TokenType.BLOCK_GROUP_OPEN_START, 'if'], [TokenType.BLOCK_PARAMETER, 'b'],   [TokenType.BLOCK_GROUP_OPEN_END],e.TEXT, 'hello b'],   [TokenType.BLOCK_OPEN_START, 'else'],     [TokenType.BLOCK_OPEN_END],   [TokenType.BLOCK_GROUP_OPEN_START, 'if'], [TokenType.BLOCK_PARAMETER, 'c'],   [TokenType.BLOCK_GROUP_OPEN_END],e.TEXT, 'hello c'],   [TokenType.BLOCK_GROUP_CLOSE, 'if'],Type.BLOCK_GROUP_CLOSE, 'if'],   [TokenType.BLOCK_GROUP_CLOSE, 'if'],Type.EOF], ]);ould parse a block group containing an expansion', () => {t result = tokenizeBlock( '{#foo}{one.two, three, =4 {four} =5 {five} foo {bar} }{/foo}', {tokenizeExpansionForms: true});ct(result).toEqual([[TokenType.BLOCK_GROUP_OPEN_START, 'foo'],[TokenType.BLOCK_GROUP_OPEN_END],[TokenType.EXPANSION_FORM_START],[TokenType.RAW_TEXT, 'one.two'],[TokenType.RAW_TEXT, 'three'],[TokenType.EXPANSION_CASE_VALUE, '=4'],[TokenType.EXPANSION_CASE_EXP_START],[TokenType.TEXT, 'four'],[TokenType.EXPANSION_CASE_EXP_END],[TokenType.EXPANSION_CASE_VALUE, '=5'],[TokenType.EXPANSION_CASE_EXP_START],[TokenType.TEXT, 'five'],[TokenType.EXPANSION_CASE_EXP_END],[TokenType.EXPANSION_CASE_VALUE, 'foo'],[TokenType.EXPANSION_CASE_EXP_START],[TokenType.TEXT, 'bar'],[TokenType.EXPANSION_CASE_EXP_END],[TokenType.EXPANSION_FORM_END],[TokenType.BLOCK_GROUP_CLOSE, 'foo'],[TokenType.EOF],    });  });}function tokenizeWithoutErrors(input: string, options?: TokenizeOptions): TokenizeResult {  const tokenizeResult = tokenize(input, 'someUrl', getHtmlTagDefinition, options);  if (tokenizeResult.errors.length > 0) {    const errorString = tokenizeResult.errors.join('\n');    throw new Error(`Unexpected parse errors:\n${errorString}`);  }  return tokenizeResult;}function humanizeParts(tokens: Token[]) {  return tokens.map(token => [token.type, ...token.parts]);}function tokenizeAndHumanizeParts(input: string, options?: TokenizeOptions): any[] {  return humanizeParts(tokenizeWithoutErrors(input, options).tokens);}function tokenizeAndHumanizeSourceSpans(input: string, options?: TokenizeOptions): any[] {  return tokenizeWithoutErrors(input, options)s.map(token => [<any>token.type, token.sourceSpan.toString()]);}function humanizeLineColumn(location: ParseLocation): string {  return `${location.line}:${location.col}`;}function tokenizeAndHumanizeLineColumn(input: string, options?: TokenizeOptions): any[] {  return tokenizeWithoutErrors(input, options)s.map(token => [<any>token.type, humanizeLineColumn(token.sourceSpan.start)]);}function tokenizeAndHumanizeFullStart(input: string, options?: TokenizeOptions): any[] {  return tokenizeWithoutErrors(input, options)s.map(token =>   [<any>token.type, humanizeLineColumn(token.sourceSpan.start),humanizeLineColumn(token.sourceSpan.fullStart)]);}function tokenizeAndHumanizeErrors(input: string, options?: TokenizeOptions): any[] {  return tokenize(input, 'someUrl', getHtmlTagDefinition, options)s.map(e => [<any>e.tokenType, e.msg, humanizeLineColumn(e.span.start)]);}